{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in /home/bastos-pc/.local/lib/python3.10/site-packages (4.3.2)\n",
      "Requirement already satisfied: nltk in /home/bastos-pc/.local/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: spacy in /home/bastos-pc/.local/lib/python3.10/site-packages (3.7.4)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/bastos-pc/.local/lib/python3.10/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/bastos-pc/.local/lib/python3.10/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/bastos-pc/.local/lib/python3.10/site-packages (from gensim) (6.4.0)\n",
      "Requirement already satisfied: joblib in /home/bastos-pc/.local/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: tqdm in /home/bastos-pc/.local/lib/python3.10/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/bastos-pc/.local/lib/python3.10/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: click in /home/bastos-pc/.local/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/bastos-pc/.local/lib/python3.10/site-packages (from spacy) (2.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/bastos-pc/.local/lib/python3.10/site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/bastos-pc/.local/lib/python3.10/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: jinja2 in /home/bastos-pc/.local/lib/python3.10/site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/bastos-pc/.local/lib/python3.10/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/bastos-pc/.local/lib/python3.10/site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/bastos-pc/.local/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/bastos-pc/.local/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/bastos-pc/.local/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/bastos-pc/.local/lib/python3.10/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /home/bastos-pc/.local/lib/python3.10/site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy) (59.6.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/bastos-pc/.local/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/bastos-pc/.local/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/bastos-pc/.local/lib/python3.10/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/bastos-pc/.local/lib/python3.10/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/bastos-pc/.local/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/bastos-pc/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/bastos-pc/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in /home/bastos-pc/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/bastos-pc/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/bastos-pc/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bastos-pc/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bastos-pc/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/bastos-pc/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/bastos-pc/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/bastos-pc/.local/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/bastos-pc/.local/lib/python3.10/site-packages (from jinja2->spacy) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim nltk spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: mwparserfromhell in /home/bastos-pc/.local/lib/python3.10/site-packages (0.6.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mwparserfromhell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercices sur word embeddings\n",
    "\n",
    "Voici  quelques  exercices  pour  renforcer  la  compréhension  des  plongements  lexicaux  (word\n",
    "embeddings) et encourager la pratique des connaissances en programmation et en TALN:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Exploration de vecteurs de mots\n",
    "\n",
    "Se familiariser avec les opérations vectorielles et la similarité sémantique.\n",
    "\n",
    "Exo : Utiliser un ensemble de vecteurs de mots pré-entraînés (par exemple, Word2Vec ou GloVe)\n",
    "pour trouver les mots les plus proches (similaires) à un mot donné.\n",
    "Par exemple, demander quel est le mot le plus similaire à \"véhicule\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors # type: ignore\n",
    "import gensim.downloader as api # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # type: ignore\n",
    "import matplotlib # type: ignore\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "from sklearn.manifold import TSNE # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "print(list(api.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle Word2Vec pré-entraîné de Google\n",
    "wv_model_path = api.load('word2vec-google-news-300', return_path=True)\n",
    "wv_model = KeyedVectors.load_word2vec_format(wv_model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercice 1\n",
      "\n",
      "Les mots les plus similaires à 'véhicule' sont :\n",
      "véhicule n'a pas de mot similaire\n",
      "\n",
      "Les mots les plus similaires à 'vehicle' sont :\n",
      "('car', 0.7821096181869507)\n",
      "('SUV', 0.7577638626098633)\n",
      "('vehicles', 0.7492177486419678)\n",
      "('minivan', 0.72130286693573)\n",
      "('Jeep', 0.704788327217102)\n",
      "\n",
      "Les mots les plus similaires à 'roi' sont :\n",
      "(\"Qu'est_ce\", 0.6547091603279114)\n",
      "('dieu', 0.6356080770492554)\n",
      "('faut', 0.6339600086212158)\n",
      "('falta', 0.6337637901306152)\n",
      "('est_très', 0.6287015080451965)\n",
      "\n",
      "Les mots les plus similaires à 'king' sont :\n",
      "('kings', 0.7138045430183411)\n",
      "('queen', 0.6510956883430481)\n",
      "('monarch', 0.6413194537162781)\n",
      "('crown_prince', 0.6204220056533813)\n",
      "('prince', 0.6159993410110474)\n",
      "\n",
      "Les mots les plus similaires à 'ciel' sont :\n",
      "('meilleur', 0.7163194417953491)\n",
      "('du_temps', 0.6971970796585083)\n",
      "('très_bien', 0.695280134677887)\n",
      "('Je_crois', 0.6943696737289429)\n",
      "(\"c'est_la\", 0.6940228343009949)\n",
      "\n",
      "Les mots les plus similaires à 'sky' sont :\n",
      "('skies', 0.6452367305755615)\n",
      "('Lightning_flickered', 0.6310141086578369)\n",
      "('heavens', 0.6219730377197266)\n",
      "('blue_sky', 0.6175693869590759)\n",
      "('Flares_lit', 0.6042904853820801)\n",
      "\n",
      "Les mots les plus similaires à 'mort' sont :\n",
      "(\"c'est_le\", 0.6823813319206238)\n",
      "(\"n'est_pas\", 0.6676763892173767)\n",
      "('ne_pas', 0.6655068397521973)\n",
      "(\"Qu'est_ce\", 0.6601776480674744)\n",
      "('quoi', 0.6590304970741272)\n",
      "\n",
      "Les mots les plus similaires à 'death' sont :\n",
      "('deaths', 0.6453005075454712)\n",
      "('murder', 0.6415225267410278)\n",
      "('untimely_death', 0.6404935121536255)\n",
      "('slaying', 0.6001620888710022)\n",
      "('killing', 0.5760149955749512)\n"
     ]
    }
   ],
   "source": [
    "print('Exercice 1')\n",
    "#Exercice 1\n",
    "def mots_similaires(mot, topn=5):\n",
    "    try:\n",
    "        mots_approchant = wv_model.most_similar(mot, topn=topn)\n",
    "        return mots_approchant\n",
    "    except KeyError:\n",
    "        return f\"{mot} n'a pas de mot similaire\"\n",
    "\n",
    "mot_recherche = [\"véhicule\",\"vehicle\",\"roi\",\"king\",\"ciel\",\"sky\",\"mort\",\"death\"]\n",
    "for mot in mot_recherche:\n",
    "    resultats = mots_similaires(mot)\n",
    "    print(f\"\\nLes mots les plus similaires à '{mot}' sont :\")\n",
    "    if isinstance(resultats, str):\n",
    "        print(resultats)\n",
    "    else:\n",
    "        for similarite in resultats:\n",
    "            print(f\"{similarite}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut voir que le model n'est pas adapté au français."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercice 2 \n",
      "\n",
      "Complétion de l'analogie : 'Paris' est à 'France' comme 'Rome' est à '[('Italy', 0.7115296125411987)]\n",
      "Complétion de l'analogie : 'Berlin' est à 'Germany' comme 'Madrid' est à '[('Spain', 0.7713854908943176)]\n",
      "Complétion de l'analogie : 'Tokyo' est à 'Japan' comme 'Beijing' est à '[('China', 0.8058403134346008)]\n"
     ]
    }
   ],
   "source": [
    "print('Exercice 2 \\n')\n",
    "# Exercice 2\n",
    "def complete_analogy(word_a, word_b, word_c, topn=1):\n",
    "    try:\n",
    "        vec_composite = (wv_model[word_b] - wv_model[word_a]) + wv_model[word_c]\n",
    "        mots_similaires = wv_model.similar_by_vector(vec_composite, topn=topn)\n",
    "        return mots_similaires\n",
    "    except KeyError:\n",
    "        return f\"L'un des mots '{word_a}', '{word_b}' ou '{word_c}' n'est pas présent dans le vocabulaire.\"\n",
    "\n",
    "\n",
    "villes_Pays = [\n",
    "    (\"Paris\", \"France\", \"Rome\"),\n",
    "    (\"Berlin\", \"Germany\", \"Madrid\"),\n",
    "    (\"Tokyo\", \"Japan\", \"Beijing\"),\n",
    "]\n",
    "\n",
    "for contenue in villes_Pays:\n",
    "    mot_a, mot_b, mot_c = contenue\n",
    "    resultats = complete_analogy(mot_a, mot_b, mot_c)\n",
    "    if isinstance(resultats, str):\n",
    "        print(resultats)\n",
    "    else:\n",
    "        print(f\"Complétion de l'analogie : '{mot_a}' est à '{mot_b}' comme '{mot_c}' est à '{resultats}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercice 3 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20856/4250160143.py:27: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "print('Exercice 3 \\n')\n",
    "\n",
    "# Obtenir les vecteurs de mots et les mots à annoter\n",
    "vecteurs_mots = mot_recherche\n",
    "vecteurs_mots.append(\"pineapple\")\n",
    "vecteurs_mots.append(\"pear\")\n",
    "vecteurs_mots.append(\"Paris\")\n",
    "vecteurs_mots.append(\"Berlin\")\n",
    "tsne = TSNE(n_components=2, perplexity=5, random_state=42)\n",
    "words_to_visualize = [mot for mot in vecteurs_mots if mot in wv_model.key_to_index]\n",
    "word_vectors = [wv_model[mot] for mot in words_to_visualize]\n",
    "word_vectors_np = np.array(word_vectors)\n",
    "word_vectors_2d = tsne.fit_transform(word_vectors_np)\n",
    "\n",
    "# Visualisation des mots dans l'espace 2D\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1], marker='o')\n",
    "\n",
    "# Ajout des étiquettes pour chaque mot\n",
    "for i, word in enumerate(words_to_visualize):\n",
    "    plt.annotate(word, xy=(word_vectors_2d[i, 0], word_vectors_2d[i, 1]), fontsize=12)\n",
    "\n",
    "plt.title('Projection des mots dans l\\'espace 2D avec t-SNE')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.savefig('figure.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # type: ignore\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # type: ignore\n",
    "from sklearn.model_selection import train_test_split # type: ignore\n",
    "from sklearn.linear_model import LogisticRegression # type: ignore\n",
    "from sklearn.metrics import classification_report # type: ignore\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_dataset():\n",
    "    # Listes d'avis factices\n",
    "    avis_positifs = [\n",
    "        \"Ce jeu est incroyable, je ne peux pas m'arrêter d'y jouer !\",\n",
    "        \"Des heures de plaisir garanties avec ce jeu.\",\n",
    "        \"Les développeurs ont vraiment fait du bon travail avec ce jeu.\",\n",
    "        \"Je recommande ce jeu à tous mes amis, c'est un vrai bijou !\",\n",
    "        \"Une expérience de jeu immersive et captivante.\",\n",
    "        \"Ce jeu est incroyable !\",\n",
    "        \"J'ai adoré ce jeu, les graphismes sont superbes.\",\n",
    "        \"Très bon gameplay, je recommande vivement !\",\n",
    "        \"Un must-have pour tous les fans de jeux vidéo.\",\n",
    "        \"Excellente histoire et personnages bien développés.\"\n",
    "    ]\n",
    "\n",
    "    avis_negatifs = [\n",
    "        \"Évitez ce jeu à tout prix, c'est une véritable catastrophe.\",\n",
    "        \"Je me suis ennuyé dès les premières minutes de jeu.\",\n",
    "        \"Les bugs et les problèmes de performance rendent ce jeu injouable.\",\n",
    "        \"Un gaspillage d'argent complet, je regrette mon achat.\",\n",
    "        \"Ce jeu est tellement mauvais que je l'ai désinstallé après quelques minutes.\",\n",
    "        \"Je suis très déçu par ce jeu, il est ennuyeux.\",\n",
    "        \"Les contrôles sont médiocres, je ne recommande pas.\",\n",
    "        \"Graphismes décevants, le jeu est daté.\",\n",
    "        \"J'ai trouvé ce jeu très frustrant et mal conçu.\",\n",
    "        \"Une grosse déception, je regrette mon achat.\"\n",
    "    ]\n",
    "\n",
    "    # Création du jeu de données\n",
    "    avis = []\n",
    "    labels = []\n",
    "\n",
    "    # Ajout d'avis positifs\n",
    "    for _ in range(50):\n",
    "        avis.append(random.choice(avis_positifs))\n",
    "        labels.append('+')\n",
    "\n",
    "    # Ajout d'avis négatifs\n",
    "    for _ in range(50):\n",
    "        avis.append(random.choice(avis_negatifs))\n",
    "        labels.append('-')\n",
    "\n",
    "    # Mélange des données\n",
    "    data = list(zip(avis, labels))\n",
    "    random.shuffle(data)\n",
    "\n",
    "    # Création du DataFrame pandas\n",
    "    df = pd.DataFrame(data, columns=['Avis', 'Sentiment'])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df['Avis'], df['Test'], test_size=0.2, random_state=42)\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "    modele = LogisticRegression()\n",
    "    modele.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    predictions = modele.predict(X_test_tfidf)\n",
    "    print(classification_report(y_test, predictions))\n",
    "\n",
    "    return vectorizer, modele\n",
    "\n",
    "def predict_sentiment(vectorizer, modele, texte):\n",
    "    texte_tfidf = vectorizer.transform([texte])\n",
    "    prediction = modele.predict(texte_tfidf)\n",
    "\n",
    "    if prediction[0] == '+':\n",
    "        return \"positif\"\n",
    "    else:\n",
    "        return \"négatif\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercice 4 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           +       0.87      0.96      0.91       231\n",
      "           -       0.93      0.77      0.84       145\n",
      "\n",
      "    accuracy                           0.89       376\n",
      "   macro avg       0.90      0.86      0.87       376\n",
      "weighted avg       0.89      0.89      0.88       376\n",
      "\n",
      "                                                Avis Sentiment\n",
      "0             Graphismes décevants, le jeu est daté.         -\n",
      "1       Une grosse déception, je regrette mon achat.         -\n",
      "2  Évitez ce jeu à tout prix, c'est une véritable...         -\n",
      "3  Évitez ce jeu à tout prix, c'est une véritable...         -\n",
      "4        Très bon gameplay, je recommande vivement !         +\n",
      "Pourcentage de réussite : 100.0%\n"
     ]
    }
   ],
   "source": [
    "print('Exercice 4 \\n')\n",
    "\n",
    "debug = False\n",
    "total = 0\n",
    "nb_erreurs = 0\n",
    "vectorizer, modele = train_model(\"label_avis.csv\")\n",
    "\n",
    "# Exemple d'utilisation de la fonction\n",
    "jeux_video_df = generate_dataset()\n",
    "print(jeux_video_df.head())\n",
    "for _, row in jeux_video_df.iterrows():\n",
    "    total += 1\n",
    "    prediction = predict_sentiment(vectorizer, modele, row['Avis'])\n",
    "    if prediction != row['Sentiment']:\n",
    "        nb_erreurs += 1\n",
    "    if debug :print(f\"Le sentiment du texte est {prediction}. Étiquette réelle : {row['Sentiment']}\")\n",
    "\n",
    "pourcentage_erreurs = (nb_erreurs / total) * 100\n",
    "print(f\"Pourcentage de réussite : {pourcentage_erreurs}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le scraper sera disponible dans mon projet. (a l'heure actuelle je n'ai pas de wifi donc je ne vais pas m'amuser a scraper les donner en 4g j'utilise donc ma base de donner obtenu en amont 😅)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bastos-pc/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['review', 'label', 'llama_training_data', '__index_level_0__'],\n",
      "        num_rows: 40629\n",
      "    })\n",
      "})\n",
      "l'intrigue était prometteuse mais des scènes sans aucune crédibilité s'enchainent sur une intrigue qui ne tient rapidement plus la route faisant de ce film un nanard à gros budget\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('SergeiZu/french-film-reviews')\n",
    "reviews = [review['review'] for review in dataset['train']]\n",
    "print(dataset)\n",
    "print(reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Test                                               Avis\n",
      "0       +  ea qui servira à faire un jeux nickel pour sa ...\n",
      "1       +  vivement que la version finale soit fin prête ...\n",
      "2       +  deux heures après le démarrage de l'ea du jeu,...\n",
      "3       +  une note relativement élevée afin de , je l'av...\n",
      "4       +  mon avis s'adresse au haters.je fais partie de...\n",
      "...   ...                                                ...\n",
      "1872    -  0 malgres que j'attendais ce jeu et fan des ba...\n",
      "1873    -  quelle déception venant de la part d'un studio...\n",
      "1874    -  gameplay ultra répétitif, graphismes qui n’ont...\n",
      "1875    -  tellement triste que rocksteady soit passés de...\n",
      "1876    -  étant un fan des jeux arkham, je me suis laiss...\n",
      "\n",
      "[1877 rows x 2 columns]\n",
      "ea qui servira à faire un jeux nickel pour sa sortie !on est de suite dans l’ambiance d&d.pour le moment, c’est une bonne surprise... vivement la version gold !\n"
     ]
    }
   ],
   "source": [
    "# Charger le corpus exercice4_cleaned.csv\n",
    "exercice4_df = pd.read_csv('label_avis.csv')\n",
    "exercice4_reviews = exercice4_df['Avis'].tolist()\n",
    "print(exercice4_df)\n",
    "print(exercice4_reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement du modèle Word2Vec sur le corpus Allociné\n",
    "corpus1_sentences = [review.split() for review in reviews]\n",
    "model1 = Word2Vec(corpus1_sentences, vector_size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "# Entraînement du modèle Word2Vec sur le corpus exercice4_cleaned.csv\n",
    "corpus2_sentences = [review.split() if isinstance(review, str) else [] for review in exercice4_reviews]\n",
    "model2 = Word2Vec(corpus2_sentences, vector_size=100, window=5, min_count=5, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des vecteurs de mots pour les mots communs\n",
    "common_words = set(model1.wv.index_to_key) & set(model2.wv.index_to_key)\n",
    "word_differences = {}\n",
    "for word in common_words:\n",
    "    vector1 = model1.wv[word]\n",
    "    vector2 = model2.wv[word]\n",
    "    difference = np.linalg.norm(vector1 - vector2)\n",
    "    word_differences[word] = difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercice 4 \n",
      "\n",
      "Mots dont les vecteurs ont le plus changé 😊\n",
      "je: 17.18136978149414\n",
      "sont: 15.19853687286377\n",
      "il: 14.825483322143555\n",
      "j'ai: 14.630622863769531\n",
      "vous: 14.618936538696289\n",
      "on: 14.613677978515625\n",
      "y: 14.56956672668457\n",
      "suis: 14.348804473876953\n",
      "ne: 13.56860065460205\n",
      "pas: 13.154099464416504\n"
     ]
    }
   ],
   "source": [
    "print('Exercice 4 \\n')\n",
    "\n",
    "# Identification des mots dont les vecteurs ont le plus changé\n",
    "most_changed_words = sorted(word_differences, key=word_differences.get, reverse=True)[:10]\n",
    "print(\"Mots dont les vecteurs ont le plus changé 😊\")\n",
    "for word in most_changed_words:\n",
    "    print(f\"{word}: {word_differences[word]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
